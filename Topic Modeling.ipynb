{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"SenateTweets.csv\", parse_dates=[\"TweetDate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added the code above as a way of dealing with writing over data frames as we clean our data. Anytime we want to process a data frame with some cleaning algorithm, we may not care about the old data and so will want to simply write-over the old data frame. Although it is not always a best practice to write over old data with new data, it is often more efficient for memory and so I simply suggest using your own discretion. Using the above code will stop Pandas from printing a warning to this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>UserID</th>\n",
       "      <th>...</th>\n",
       "      <th>Rep_candidate</th>\n",
       "      <th>Ind_candidate</th>\n",
       "      <th>Winner</th>\n",
       "      <th>Loser</th>\n",
       "      <th>Female_candidate</th>\n",
       "      <th>Male_candidate</th>\n",
       "      <th>Gender_Mentioned</th>\n",
       "      <th>Trump</th>\n",
       "      <th>Biden</th>\n",
       "      <th>prez_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5816292</td>\n",
       "      <td>[]</td>\n",
       "      <td>1308194337264513024</td>\n",
       "      <td>2020-09-22 00:00:09+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Cory Gardner just said he'll vote to fill Ruth...</td>\n",
       "      <td>((( PJ ))</td>\n",
       "      <td>pawsupbuttdown</td>\n",
       "      <td>352780384</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5816295</td>\n",
       "      <td>[]</td>\n",
       "      <td>1308194337684107265</td>\n",
       "      <td>2020-09-22 00:00:09+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Cory Gardner just said he'll vote to fill Ruth...</td>\n",
       "      <td>brrrgrrr</td>\n",
       "      <td>BurrerMelissa</td>\n",
       "      <td>1254135911396388867</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5816298</td>\n",
       "      <td>[]</td>\n",
       "      <td>1308194337780412416</td>\n",
       "      <td>2020-09-22 00:00:09+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Cory Gardner just said he'll vote to fill Ruth...</td>\n",
       "      <td>ðŸŒŠBlue Tsunami</td>\n",
       "      <td>CeCeChattter</td>\n",
       "      <td>4786062176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5816301</td>\n",
       "      <td>[]</td>\n",
       "      <td>1308194338061586433</td>\n",
       "      <td>2020-09-22 00:00:09+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>@SenCoryGardner FACT CHECK: February 18, 2016 ...</td>\n",
       "      <td>American NinaðŸŒ»</td>\n",
       "      <td>Thenina77</td>\n",
       "      <td>2387540972</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>5816308</td>\n",
       "      <td>[]</td>\n",
       "      <td>1308194338900455424</td>\n",
       "      <td>2020-09-22 00:00:10+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>In Trump they trust.   cc: @CoryGardner, @sena...</td>\n",
       "      <td>jckthesword</td>\n",
       "      <td>jckthesword</td>\n",
       "      <td>14616404</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1 Hashtags              TweetID  \\\n",
       "0           1       5816292       []  1308194337264513024   \n",
       "1           4       5816295       []  1308194337684107265   \n",
       "2           7       5816298       []  1308194337780412416   \n",
       "3          10       5816301       []  1308194338061586433   \n",
       "4          17       5816308       []  1308194338900455424   \n",
       "\n",
       "                  TweetDate  Retweet  \\\n",
       "0 2020-09-22 00:00:09+00:00     True   \n",
       "1 2020-09-22 00:00:09+00:00     True   \n",
       "2 2020-09-22 00:00:09+00:00     True   \n",
       "3 2020-09-22 00:00:09+00:00     True   \n",
       "4 2020-09-22 00:00:10+00:00     True   \n",
       "\n",
       "                                           TweetText        UserName  \\\n",
       "0  Cory Gardner just said he'll vote to fill Ruth...       ((( PJ ))   \n",
       "1  Cory Gardner just said he'll vote to fill Ruth...        brrrgrrr   \n",
       "2  Cory Gardner just said he'll vote to fill Ruth...   ðŸŒŠBlue Tsunami   \n",
       "3  @SenCoryGardner FACT CHECK: February 18, 2016 ...  American NinaðŸŒ»   \n",
       "4  In Trump they trust.   cc: @CoryGardner, @sena...     jckthesword   \n",
       "\n",
       "       ScreenName               UserID  ... Rep_candidate Ind_candidate  \\\n",
       "0  pawsupbuttdown            352780384  ...             0             0   \n",
       "1   BurrerMelissa  1254135911396388867  ...             0             0   \n",
       "2    CeCeChattter           4786062176  ...             0             0   \n",
       "3       Thenina77           2387540972  ...             1             0   \n",
       "4     jckthesword             14616404  ...             1             0   \n",
       "\n",
       "   Winner  Loser  Female_candidate Male_candidate  Gender_Mentioned  Trump  \\\n",
       "0       1      0                 0              1                 M      0   \n",
       "1       1      0                 0              1                 M      0   \n",
       "2       1      0                 0              1                 M      0   \n",
       "3       0      1                 0              1                 M      0   \n",
       "4       1      0                 0              1                 M      0   \n",
       "\n",
       "   Biden prez_mentioned  \n",
       "0      0              N  \n",
       "1      0              N  \n",
       "2      0              N  \n",
       "3      0              N  \n",
       "4      0              N  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'TweetText'\n",
    "#group_col = 'gender'\n",
    "#df_text = df[[group_col, text_col]]\n",
    "\n",
    "df_text = df[[text_col]]\n",
    "\n",
    "df_text[text_col] = df_text[text_col].replace(to_replace=r'[ , | ? | $ | . | ! | - | : ]' , value = r'', regex = True)\n",
    "df_text[text_col] = df_text[text_col].replace(to_replace=r'[ ^a-zA-Z ] ', value = r' ', regex = True)\n",
    "df_text[text_col] = df_text[text_col].replace(to_replace=r'\\s\\s+' , value = r' ', regex = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next set of code we first identify the column that contains our grouper like gender. We then identify our column that contains our text data. Once we have identified our two most important columns we create a new data frame of just those columns called df_text.\n",
    "\n",
    "Finally, I have included 3 different sets of code for doing some initial processing of the text data using Regex functions. The first function replaces funny symbols with nothing in order to remove funny symbols from analysis. You can add more symbols that may be unique to your data set by adding a | and then the symbol after. The second regex function replaces all non-letters with a space. The last regex pattern removes extra blank spaces and replaces them with a single space to ensure that each word only contains one space to the next word. These obviously have overlapping effects so use one, all, or modify to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SenCoryGardnerFACTCHECKFebruary182016inYOURow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InTrumptheytrustcc@CoryGardner@senatemajldr@Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@LindseyGrahamSChttps//tco/gJq3lSlvHc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OKAmericaCoryGardnerhasannouncedheisall-infora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RobDaly19@MNNASS1sun@Jimborobo@LindseyGrahamS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@sebulia1@CatSkoor@commons96055467@LindseyGrah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@LindseyGrahamSCIagreeButtheymadesureyoupaidfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText\n",
       "0  CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...\n",
       "1  CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...\n",
       "2  CoryGardnerjustsaidhe'llvotetofillRuthBaderGin...\n",
       "3  @SenCoryGardnerFACTCHECKFebruary182016inYOURow...\n",
       "4  InTrumptheytrustcc@CoryGardner@senatemajldr@Li...\n",
       "5              @LindseyGrahamSChttps//tco/gJq3lSlvHc\n",
       "6  OKAmericaCoryGardnerhasannouncedheisall-infora...\n",
       "7  @RobDaly19@MNNASS1sun@Jimborobo@LindseyGrahamS...\n",
       "8  @sebulia1@CatSkoor@commons96055467@LindseyGrah...\n",
       "9  @LindseyGrahamSCIagreeButtheymadesureyoupaidfo..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in data type shown below. We must convert the text column to a string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@LindseyGrahamSChttps//tco/gJq3lSlvHc'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_text = df_text.reset_index\n",
    "df_text.TweetText[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TweetText'] = df['TweetText'].astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text= [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df['text'] = df['text'].map( {'none': NaN}).astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text= [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df['text']=df['text'].dropna().inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = set(['not','n/a','na'])\n",
    "stopwords = stop - operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'):\n",
    "           return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def lemmarati(tup_list):\n",
    "    if not (np.all(pd.notnull(tup_list))):\n",
    "        return tup_list\n",
    "    outputlist = []\n",
    "    for i, j in tup_list:\n",
    "        pos = get_wordnet_pos(i)\n",
    "        lemma = wnl.lemmatize(i)\n",
    "        outputlist.append(lemma)\n",
    "    return outputlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next set of code, we are activating and setting up some functions that will allow us to do some more cleaning and normalizing of the text data. More specifically, the code sets up a function to remove stopwords, or words that are very common and as a result not all that meaningful (e.g. the). The remaining code also performs lemmatization. Lemmatization is a way of normalizing text so that words like Python, Pythons, and Pythonic all become just Python. Thus, lemmatization is like stemming but it takes the part of speech into account so that meet (v) and meeting (n) are kept separate.\n",
    "\n",
    "Also, note that before defining our stopword list we remove some words that we want to keep in our topic analysis. Words like â€˜notâ€™ although often considered a stopword, can be very important when performing topic or sentiment analysis. Consider the difference between â€˜happyâ€™ and â€˜not happy.â€™ The latter is the opposite of the former however if we used the nltk stopwords list we would remove â€˜notâ€™ from the list and run the risk of thinking most comments were â€˜happyâ€™ when in reality they were â€˜not happy.â€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df[text_col] = df[text_col].map(lambda x: nltk.word_tokenize(x.lower()) if (np.all(pd.notnull(x))) else x.lower())\n",
    "\n",
    "    df[text_col] = df[text_col].map(lambda x: pos_tag(x) if (np.all(pd.notnull(x))) else x)\n",
    "\n",
    "    df[text_col] = df[text_col].map(lemmarati)\n",
    "\n",
    "    df[text_col] = df[text_col].map(lambda x: remove_stopwords(x,stopwords) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_text[text_col] = df_text[text_col].map(lambda x: nltk.word_tokenize(x.lower()) if (np.all(pd.notnull(x))) else x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text[text_col] = df_text[text_col].map(lambda x: pos_tag(x) if (np.all(pd.notnull(x))) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text[text_col] = df_text[text_col].map(lemmarati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text[text_col] = df_text[text_col].map(lambda x: remove_stopwords(x,stopwords) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text[text_col] = df_text[text_col].map(lambda x: ' '.join(x) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_features = 1000\n",
    "n_topics= 10\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_df = .95, min_df = 2, max_features = n_features, ngram_range = (2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groups = df_text[group_col].unique()\n",
    "results = []\n",
    "\n",
    "for i in groups: \n",
    "    df_grp = adf_text.loc[adf_text[group_col] == i]\n",
    "    if len(df_grp[text_col]) > 3:\n",
    "        tf = tfidf_vec.fit_transform(df_grp[text_col])\n",
    "        feature_names = tfidf_vec.get_feature_names()\n",
    "        try:\n",
    "            nmf = NMF(n_components = n_topics, random_state=1,alpha=.1, l1_ratio=.5).fit(tf)\n",
    "            df_topics = pd.DataFrame(nmf.components_)\n",
    "            df_topics.columns = feature_names\n",
    "            df_top = df_topics.apply(lambda x: pd.Series(x.sort_values(ascending=False).iloc[:5].index,index=['top1','top2','top3','top4','top5']), axis=1).reset_index()\n",
    "            df_top['Group'] = i\n",
    "            results.append(df_top)\n",
    "        except:\n",
    "            results.append(i+' Did not produce topic results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "if len(df_text[text_col]) > 3:\n",
    "    tf = tfidf_vec.fit_transform(df_text[text_col])\n",
    "    feature_names = tfidf_vec.get_feature_names()\n",
    "    try:\n",
    "        nmf = NMF(n_components = n_topics, random_state=1,alpha=.1, l1_ratio=.5).fit(tf)\n",
    "        df_topics = pd.DataFrame(nmf.components_)\n",
    "        df_topics.columns = feature_names\n",
    "        df_top = df_topics.apply(lambda x: pd.Series(x.sort_values(ascending=False).iloc[:5].index,index=['top1','top2','top3','top4','top5']), axis=1).reset_index()\n",
    "        results.append(df_top)\n",
    "    except:\n",
    "        results.append(i+' Did not produce topic results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we first get a list of the unique groups in our grouping column. We then create a container (in this case a list) to hold our resulting data frames from the NMF topic analysis.\n",
    "\n",
    "In the for loop, we perform a separate NMF analysis for each unique group contained in the grouping column. We use the â€˜if len(df_grp[text_col]) > 100â€™ logic to ensure we have enough rows of text for the analysis. We use the â€˜try:â€™ statement to ensure that the analysis will still run in case one of the groups gives us an error. In the â€˜try:â€™ code we perform the NMF, extract the components into a data frame, label the data frame with the feature names (the bi and trigrams), selecting only the top 5 bi and trigrams for each topic based on their numeric contribution to the topic, add a column to the data frame to keep track of which group the topics are for, and append the results into our results list.\n",
    "\n",
    "Now we have a list of data frames, which are not useful as a list so one more step before we finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = pd.concat(results,axis=0)\n",
    "topic_results.to_csv('C:/Users/student/Desktop/my_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https tco</td>\n",
       "      <td>lindseygrahamsc https</td>\n",
       "      <td>joebiden amp</td>\n",
       "      <td>senatorloeffler senmikelee</td>\n",
       "      <td>justsentanother100to harrisonjaime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lindseygrahamschttps tco</td>\n",
       "      <td>tco flxakztyeq</td>\n",
       "      <td>agreed lindseygrahamschttps tco</td>\n",
       "      <td>agreed lindseygrahamschttps</td>\n",
       "      <td>lindseygrahamschttps tco flxakztyeq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>harrisonjaimehttps tco</td>\n",
       "      <td>pleasedon tmakeitworseforgrahambygivingtohisop...</td>\n",
       "      <td>stevebullockmtnc calforncsc</td>\n",
       "      <td>stevebullockmtnc calforncsc harrisonjaimehttps</td>\n",
       "      <td>calforncsc harrisonjaimehttps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>amymcgrathkyhttps tco</td>\n",
       "      <td>thefailedvotehimout amymcgrathkyhttps</td>\n",
       "      <td>thefailedvotehimout amymcgrathkyhttps tco</td>\n",
       "      <td>kentuckyyouarethe5thpooreststateintheunionnoma...</td>\n",
       "      <td>kentuckyyouarethe5thpooreststateintheunionnoma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>itwouldbemeanforpeopletosharethisandtag lindse...</td>\n",
       "      <td>ifyouareoneofthe75millionamericanswhovotedfor ...</td>\n",
       "      <td>it snotjustthatyou</td>\n",
       "      <td>it snotjustscotuschairman lindseygrahamschassc...</td>\n",
       "      <td>it snotjustscotuschairman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               top1  \\\n",
       "0      0                                          https tco   \n",
       "1      1                           lindseygrahamschttps tco   \n",
       "2      2                             harrisonjaimehttps tco   \n",
       "3      3                              amymcgrathkyhttps tco   \n",
       "4      4  itwouldbemeanforpeopletosharethisandtag lindse...   \n",
       "\n",
       "                                                top2  \\\n",
       "0                              lindseygrahamsc https   \n",
       "1                                     tco flxakztyeq   \n",
       "2  pleasedon tmakeitworseforgrahambygivingtohisop...   \n",
       "3              thefailedvotehimout amymcgrathkyhttps   \n",
       "4  ifyouareoneofthe75millionamericanswhovotedfor ...   \n",
       "\n",
       "                                        top3  \\\n",
       "0                               joebiden amp   \n",
       "1            agreed lindseygrahamschttps tco   \n",
       "2                stevebullockmtnc calforncsc   \n",
       "3  thefailedvotehimout amymcgrathkyhttps tco   \n",
       "4                         it snotjustthatyou   \n",
       "\n",
       "                                                top4  \\\n",
       "0                         senatorloeffler senmikelee   \n",
       "1                        agreed lindseygrahamschttps   \n",
       "2     stevebullockmtnc calforncsc harrisonjaimehttps   \n",
       "3  kentuckyyouarethe5thpooreststateintheunionnoma...   \n",
       "4  it snotjustscotuschairman lindseygrahamschassc...   \n",
       "\n",
       "                                                top5  \n",
       "0                 justsentanother100to harrisonjaime  \n",
       "1                lindseygrahamschttps tco flxakztyeq  \n",
       "2                      calforncsc harrisonjaimehttps  \n",
       "3  kentuckyyouarethe5thpooreststateintheunionnoma...  \n",
       "4                          it snotjustscotuschairman  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it snotjustthatyou'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.top3[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
